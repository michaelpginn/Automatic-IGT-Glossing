{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8cba48-0d10-4dc4-97e2-aa6013a3f2d3",
   "metadata": {},
   "source": [
    "## Basic Model Training\n",
    "Let's get our data into a format that we can use to train transformer-style models.\n",
    "\n",
    "We will start with the Korean dataset, as it is well-annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112cae38-79a3-401e-bbc2-9d075625c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xigt in /Users/milesper/miniforge3/lib/python3.10/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e993ab-75ec-414e-bf97-c889c40210f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xigt.codecs import xigtxml\n",
    "corpus = xigtxml.load(open('../data/kor.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247475e5-92cd-4837-977f-9b40752cc20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ani', ',', 'caki-ka', 'cikcep', 'o-ess-ta', '.'],\n",
       " ['No', ',', 'self', 'came', 'in', 'person', '.'],\n",
       " ['no', 'self-NOM', 'in-person', 'come-PAST-DEC']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MissingValueError(Exception):\n",
    "    pass\n",
    "\n",
    "# From a single line of IGT, extracts the features which are allowed in this shared task:\n",
    "# 1. Transcribed words (not segmented)\n",
    "# 2. Translation (not aligned)\n",
    "# 3. Glosses\n",
    "def extract_igt(igt):\n",
    "    if not igt.get('w'):\n",
    "        raise MissingValueError(\"words\")\n",
    "    if not igt.get('tw'):\n",
    "        raise MissingValueError(\"translation\")\n",
    "    if not igt.get('gw'):\n",
    "        raise MissingValueError(\"glosses\")\n",
    "        \n",
    "    words = [word.value() for word in igt['w'].items]\n",
    "    glosses = [gloss.value() for gloss in igt['gw'].items]\n",
    "    \n",
    "    translation = [item.value() for item in igt['tw']]\n",
    "    return [words, translation, glosses]\n",
    "    \n",
    "extract_igt(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a7f38d-f50f-48b7-b744-a9cf339c8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed corpus, with \n",
      "\t4839 good rows\n",
      "\t73 rows missing words        \n",
      "\t471 missing translations\n",
      "\t0 missing glosses\n"
     ]
    }
   ],
   "source": [
    "corpus_data = []\n",
    "\n",
    "missing_words_count = 0\n",
    "missing_translation_count = 0\n",
    "missing_gloss_count = 0\n",
    "all_good_count = 0\n",
    "\n",
    "for i, igt in enumerate(corpus):\n",
    "    try:\n",
    "        igt_data = extract_igt(igt)\n",
    "        corpus_data.append(igt_data)\n",
    "        all_good_count += 1\n",
    "    except MissingValueError as v:\n",
    "        match str(v):\n",
    "            case 'words': missing_words_count += 1\n",
    "            case 'translation': missing_translation_count += 1\n",
    "            case 'glosses': missing_gloss_count += 1\n",
    "            case 'alignments': missing_aligments_count += 1\n",
    "    \n",
    "print(f\"Parsed corpus, with \\n\\t{all_good_count} good rows\\n\\t{missing_words_count} rows missing words\\\n",
    "        \\n\\t{missing_translation_count} missing translations\\n\\t{missing_gloss_count} missing glosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6120a391-07e6-4a50-9959-06b016e5289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chelsu-nun', 'pam-ul', 'kuw-e', 'mek-ess-ta', '.'],\n",
       " ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "752f6266-bec9-4525-a7d4-eab2b116cd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove the dashes from the input, to simulate the case where we don't have segmentation\n",
    "for item in corpus_data:\n",
    "    for i, word in enumerate(item[0]):\n",
    "        item[0][i] = word.replace('-', '')\n",
    "        \n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a305452e-66c6-411e-b095-e22a8e5e50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " ['Top', 'chestnut', '-Acc', 'broil', '-Inf', 'eat', '-Past', '-Dec']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also split the output by dashes\n",
    "for item in corpus_data:\n",
    "    glosses = []\n",
    "    for i, word in enumerate(item[2]):\n",
    "        word_glosses = word.split(\"-\")\n",
    "        glosses.append(word_glosses[0])\n",
    "        glosses += [\"-\" + gloss for gloss in word_glosses[1:]]\n",
    "    item[2] = glosses\n",
    "\n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d75d8-1f32-4dd1-b27b-4092d5b1bf24",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- We originally tried to align words and glosses, but it turns out a huge number of rows are either missing alignments, or have completely wrong alignments. Rather than mess up our model with incorrect data, we will simply provide unaligned glosses.\n",
    "- There's a lot of messy unnecessary data. We will have to count on the transformer to deal with those.\n",
    "\n",
    "# Encoding\n",
    "Input: transcription + translation\n",
    "\n",
    "Output: glosses (stems and grams)\n",
    "\n",
    "We need to encode all of our items, input and output, as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4840ca3-f319-453e-8004-faad3611b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "special_chars = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\n",
    "def create_vocab(sentences: List[List[str]], threshold=2):\n",
    "    all_words = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            all_words[word.lower()] = all_words.get(word.lower(), 0) + 1\n",
    "\n",
    "    all_words_list = []\n",
    "    for word, count in all_words.items():\n",
    "        if count >= threshold:\n",
    "            all_words_list.append(word)\n",
    "\n",
    "    return sorted(all_words_list)\n",
    "\n",
    "source_vocab = create_vocab([item[0] for item in corpus_data])\n",
    "len(source_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b71530-6266-4400-9408-a026d66fa28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3357\n"
     ]
    }
   ],
   "source": [
    "# Also create a list for the target and gloss words\n",
    "target_and_gloss_vocab = create_vocab([item[1] for item in corpus_data] + [item[2] for item in corpus_data])\n",
    "print(len(target_and_gloss_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17603cd6-05f6-49d6-8218-afdc7fb9494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3175"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_word(word, vocab='source'):\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word in special_chars:\n",
    "        return special_chars.index(word)\n",
    "    if vocab=='source':\n",
    "        if word in source_vocab:\n",
    "            return source_vocab.index(word) + len(special_chars)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if word in target_and_gloss_vocab:\n",
    "            return target_and_gloss_vocab.index(word) + len(special_chars) + len(source_vocab)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "encode_word('', vocab='transl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ba05e2-00e2-4569-ba2a-5ca5652ec291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 481, 2143, 1532, 1721,   57,    1, 4259, 4160, 3917, 3976, 6189, 4264,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_INPUT_LENGTH = 512\n",
    "\n",
    "PAD_ID = special_chars.index(\"[PAD]\")\n",
    "SEP_ID = special_chars.index(\"[SEP]\")\n",
    "\n",
    "# Encodes a sentence as integers, and pads it\n",
    "def encode(sentence: List[str], vocab='source') -> List[int]:\n",
    "    return [encode_word(word, vocab=vocab) for word in sentence]\n",
    "            \n",
    "    \n",
    "def encode_item(item) -> List[int]:\n",
    "    \"\"\"Encodes an item from the data set by combining the source and target\"\"\"\n",
    "    source_enc = encode(item[0])\n",
    "    target_enc = encode(item[1], vocab='transl')\n",
    "    combined_enc = source_enc + [SEP_ID] + target_enc\n",
    "    \n",
    "    # Pad\n",
    "    initial_length = len(combined_enc)\n",
    "    combined_enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = [1] * initial_length + [0] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    return {'input_ids': torch.tensor(combined_enc), 'attention_mask': torch.tensor(attention_mask)}\n",
    "    \n",
    "encode_item(corpus_data[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0616eb24-d576-424e-a00f-72d146c7c512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  17,   79,   20,  ...,    2,    2,    2],\n",
       "         [ 242,   55,  339,  ...,    2,    2,    2],\n",
       "         [ 154,   57,  481,  ...,    2,    2,    2],\n",
       "         [ 295,   57, 1400,  ...,    2,    2,    2],\n",
       "         [ 481, 2143, 1532,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_batch(batch: List[List[List[str]]]):\n",
    "    \"\"\"Input should be a list of lists, where the first item is the source and the second is the translation\"\"\"\n",
    "    all_encoded = {'input_ids': [], 'attention_mask': []}\n",
    "    \n",
    "    for item in batch:\n",
    "        encoded = encode_item(item)\n",
    "        all_encoded['input_ids'].append(encoded['input_ids'])\n",
    "        all_encoded['attention_mask'].append(encoded['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.stack(all_encoded['input_ids'])\n",
    "    attention_mask = torch.stack(all_encoded['attention_mask'])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "encode_batch(corpus_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3149336d-a4c0-400e-ac8d-f0d0b0fdd7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   4, 3175, 3525,  ...,    2,    2,    2],\n",
      "        [   4, 5421, 5862,  ...,    2,    2,    2],\n",
      "        [   4, 4259, 3683,  ...,    2,    2,    2],\n",
      "        [   4, 4705, 3525,  ...,    2,    2,    2],\n",
      "        [   4, 6265, 4264,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "BOS_ID = special_chars.index(\"[BOS]\")\n",
    "EOS_ID = special_chars.index(\"[EOS]\")\n",
    "\n",
    "def encode_gloss_batch(batch: List[List[str]]):\n",
    "    \"\"\"Encodes an output batch. Each item should be a list of output gloss words.\"\"\"\n",
    "    all_encoded = {'input_ids': [], 'attention_mask': []}\n",
    "    for item in batch:\n",
    "        enc = encode(item, vocab='transl')\n",
    "        enc = [BOS_ID] + enc + [EOS_ID]\n",
    "        initial_length = len(enc)\n",
    "        enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * initial_length + [0] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "        all_encoded['input_ids'].append(torch.tensor(enc))\n",
    "        all_encoded['attention_mask'].append(torch.tensor(attention_mask))\n",
    "        \n",
    "    return {'input_ids': torch.stack(all_encoded['input_ids']), 'attention_mask': torch.stack(all_encoded['attention_mask'])}\n",
    "\n",
    "print(encode_gloss_batch([item[2] for item in corpus_data[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ec7f965-2eba-4f04-8ce7-f3141ad5f07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  17,   79,   20,  ...,    2,    2,    2],\n",
       "         [ 242,   55,  339,  ...,    2,    2,    2],\n",
       "         [ 154,   57,  481,  ...,    2,    2,    2],\n",
       "         ...,\n",
       "         [1169,    0,    1,  ...,    2,    2,    2],\n",
       "         [ 140, 2358, 1154,  ...,    2,    2,    2],\n",
       "         [1676,  140, 2358,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "        \n",
    "batch_size = 64\n",
    "\n",
    "train_input_batches = [encode_batch(b) for b in chunk(corpus_data, batch_size)]\n",
    "train_input_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a4de496-a3cb-4246-9c78-50628321f0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   4, 3175, 3525,  ...,    2,    2,    2],\n",
       "         [   4, 5421, 5862,  ...,    2,    2,    2],\n",
       "         [   4, 4259, 3683,  ...,    2,    2,    2],\n",
       "         ...,\n",
       "         [   4, 5077, 3425,  ...,    2,    2,    2],\n",
       "         [   4, 5077, 3525,  ...,    2,    2,    2],\n",
       "         [   4, 5278, 3525,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same for output\n",
    "train_output_batches = [encode_gloss_batch(b) for b in chunk([item[2] for item in corpus_data], batch_size)]\n",
    "train_output_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4339a5d-6a3c-4e98-a9b7-dab37f4e798d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
