{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8cba48-0d10-4dc4-97e2-aa6013a3f2d3",
   "metadata": {},
   "source": [
    "## Basic Model Training\n",
    "Let's get our data into a format that we can use to train transformer-style models.\n",
    "\n",
    "We will start with the Korean dataset, as it is well-annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112cae38-79a3-401e-bbc2-9d075625c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xigt in /Users/milesper/miniforge3/lib/python3.10/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e993ab-75ec-414e-bf97-c889c40210f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xigt.codecs import xigtxml\n",
    "corpus = xigtxml.load(open('../data/kor.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "247475e5-92cd-4837-977f-9b40752cc20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['a',\n",
       "  '.',\n",
       "  'John-i',\n",
       "  'koki-lul',\n",
       "  'kuw-e',\n",
       "  'mek-ki-nun',\n",
       "  'kuw-e',\n",
       "  'mek-ess-ciman'],\n",
       " 'translation': ['John',\n",
       "  'broiled',\n",
       "  'and',\n",
       "  'ate',\n",
       "  'the',\n",
       "  'meat',\n",
       "  ',',\n",
       "  'but',\n",
       "  '...'],\n",
       " 'glosses': ['Nom',\n",
       "  'meat-Acc',\n",
       "  'broil-Inf',\n",
       "  'eat-Noml-Top',\n",
       "  'broil-Inf',\n",
       "  'eat-Past-but'],\n",
       " 'alignments': [None, None, None, None, None, None]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MissingValueError(Exception):\n",
    "    pass\n",
    "\n",
    "# From a single line of IGT, extracts the features which are allowed in this shared task:\n",
    "# 1. Transcribed words (not segmented)\n",
    "# 2. Translation (not aligned)\n",
    "# 3. Glosses\n",
    "def extract_igt(igt):\n",
    "    if not igt.get('w'):\n",
    "        raise MissingValueError(\"words\")\n",
    "    if not igt.get('tw'):\n",
    "        raise MissingValueError(\"translation\")\n",
    "    if not igt.get('gw'):\n",
    "        raise MissingValueError(\"glosses\")\n",
    "        \n",
    "    words = [word.value() for word in igt['w'].items]\n",
    "    glosses = [gloss.value() for gloss in igt['gw'].items]\n",
    "    alignments = [gloss.alignment for gloss in igt['gw'].items]\n",
    "    \n",
    "    translation = [item.value() for item in igt['tw']]\n",
    "    return {'words': words, 'translation': translation, 'glosses': glosses, 'alignments': alignments}\n",
    "    \n",
    "extract_igt(corpus[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a7f38d-f50f-48b7-b744-a9cf339c8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed corpus, with \n",
      "\t4838 good rows\n",
      "\t73 rows missing words        \n",
      "\t471 missing translations\n",
      "\t0 missing glosses\n"
     ]
    }
   ],
   "source": [
    "corpus_data = []\n",
    "\n",
    "missing_words_count = 0\n",
    "missing_translation_count = 0\n",
    "missing_gloss_count = 0\n",
    "all_good_count = 0\n",
    "\n",
    "for i, igt in enumerate(corpus):\n",
    "    try:\n",
    "        igt_data = extract_igt(igt)\n",
    "        corpus_data.append(igt_data)\n",
    "        all_good_count += 1\n",
    "    except MissingValueError as v:\n",
    "        match str(v):\n",
    "            case 'words': missing_words_count += 1\n",
    "            case 'translation': missing_translation_count += 1\n",
    "            case 'glosses': missing_gloss_count += 1\n",
    "\n",
    "print(f\"Parsed corpus, with \\n\\t{all_good_count} good rows\\n\\t{missing_words_count} rows missing words\\\n",
    "        \\n\\t{missing_translation_count} missing translations\\n\\t{missing_gloss_count} missing glosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6120a391-07e6-4a50-9959-06b016e5289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsu-nun', 'pam-ul', 'kuw-e', 'mek-ess-ta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752f6266-bec9-4525-a7d4-eab2b116cd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove the dashes from the input, to simulate the case where we don't have segmentation\n",
    "for item in corpus_data:\n",
    "    for i, word in enumerate(item['words']):\n",
    "        item['words'][i] = word.replace('-', '')\n",
    "        \n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a305452e-66c6-411e-b095-e22a8e5e50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top',\n",
       "  'chestnut',\n",
       "  '-Acc',\n",
       "  'broil',\n",
       "  '-Inf',\n",
       "  'eat',\n",
       "  '-Past',\n",
       "  '-Dec']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also split the output by dashes\n",
    "for item in corpus_data:\n",
    "    glosses = []\n",
    "    for i, word in enumerate(item['glosses']):\n",
    "        word_glosses = word.split(\"-\")\n",
    "        glosses.append(word_glosses[0])\n",
    "        glosses += [\"-\" + gloss for gloss in word_glosses[1:]]\n",
    "    item['glosses'] = glosses\n",
    "\n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d75d8-1f32-4dd1-b27b-4092d5b1bf24",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- We originally tried to align words and glosses, but it turns out a huge number of rows are either missing alignments, or have completely wrong alignments. Rather than mess up our model with incorrect data, we will simply provide unaligned glosses.\n",
    "- There's a lot of messy unnecessary data. We will have to count on the transformer to deal with those.\n",
    "\n",
    "# Encoding\n",
    "Input: transcription + translation\n",
    "\n",
    "Output: glosses (stems and grams)\n",
    "\n",
    "We need to encode all of our items, input and output, as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5136ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_text = open('all_text.txt', 'w')\n",
    "all_text.write('')\n",
    "all_text.close()\n",
    "all_text = open('all_text.txt', 'a')\n",
    "for item in corpus_data:\n",
    "    all_text.write(\" \".join(item['words']) + \"\\n\")\n",
    "    all_text.write(\" \".join(item['translation']) + \"\\n\")\n",
    "    all_text.write(\" \".join(item['glosses']) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7f982ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./kor-vocab.json', './kor-merges.txt']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "special_chars = [\"[BOS]\", \"[EOS]\", \"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=['./all_text.txt'], min_frequency=2, special_tokens=special_chars)\n",
    "tokenizer.save_model(\".\", \"kor\")\n",
    "# tokenizer = BartTokenizer(model_max_length=512, add_prefix_space=True)\n",
    "# tokenizer.train(files=['./all_text.txt'], min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8f87527",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11399"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = BartTokenizer('./kor-vocab.json', './kor-merges.txt', bos_token=\"[BOS]\", eos_token=\"[EOS]\", sep_token=\"[SEP]\", cls_token=\"[BOS]\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", mask_token=\"[MASK]\", model_max_length=512)\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82879c11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[263, 296, 2565, 2386, 1594, 563, 345, 1237, 983, 279, 7261]\n",
      " t\n",
      "ar\n",
      "oo\n",
      "ga\n",
      " da\n",
      "ig\n",
      "ak\n",
      "uni\n",
      " de\n",
      "ka\n",
      "keta\n",
      "[' tarooga daigakuni dekaketa']\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer.encode(['tarooga', 'daigakuni', 'dekaketa'], is_split_into_words=True, add_special_tokens=False)\n",
    "print(enc)\n",
    "for tok in enc:\n",
    "    print(tokenizer.decode([tok]))\n",
    "dec = tokenizer.batch_decode([enc], clean_up_tokenization_spaces=False)\n",
    "print(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "398dec13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chelsunun pamul kuwe mekessta .'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(corpus_data[4]['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4840ca3-f319-453e-8004-faad3611b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "special_chars = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\n",
    "def create_vocab(sentences: List[List[str]], threshold=2):\n",
    "    all_words = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            all_words[word.lower()] = all_words.get(word.lower(), 0) + 1\n",
    "\n",
    "    all_words_list = []\n",
    "    for word, count in all_words.items():\n",
    "        if count >= threshold:\n",
    "            all_words_list.append(word)\n",
    "\n",
    "    return sorted(all_words_list)\n",
    "\n",
    "source_vocab = create_vocab([item['words'] for item in corpus_data])\n",
    "len(source_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19b71530-6266-4400-9408-a026d66fa28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3357\n"
     ]
    }
   ],
   "source": [
    "# Also create a list for the target and gloss words\n",
    "target_and_gloss_vocab = create_vocab([item['translation'] for item in corpus_data] + [item['glosses'] for item in corpus_data])\n",
    "print(len(target_and_gloss_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17603cd6-05f6-49d6-8218-afdc7fb9494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3175"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_word(word, vocab='source'):\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word in special_chars:\n",
    "        return special_chars.index(word)\n",
    "    if vocab=='source':\n",
    "        if word in source_vocab:\n",
    "            return source_vocab.index(word) + len(special_chars)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if word in target_and_gloss_vocab:\n",
    "            return target_and_gloss_vocab.index(word) + len(special_chars) + len(source_vocab)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "encode_word('', vocab='transl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96308c5-7265-472e-ba75-196d9327db67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[481, 2143, 1532, 1721, 57]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_INPUT_LENGTH = 512\n",
    "\n",
    "PAD_ID = special_chars.index(\"[PAD]\")\n",
    "SEP_ID = special_chars.index(\"[SEP]\")\n",
    "\n",
    "# Encodes a sentence as integers, and pads it\n",
    "def encode(sentence: List[str], vocab='source') -> List[int]:\n",
    "    return [encode_word(word, vocab=vocab) for word in sentence]\n",
    "            \n",
    "encode(corpus_data[4]['words']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d305e-0ffa-48ea-baa7-06614a0cdfc0",
   "metadata": {},
   "source": [
    "Now let's divide our data and turn it into the Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c706df-d6a0-43bf-b1b8-67734a526ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3387\n",
      "Dev: 726\n",
      "Test: 726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(corpus_data, test_size=0.3)\n",
    "test, dev = train_test_split(test, test_size=0.5)\n",
    "\n",
    "print(f\"Train: {len(train)}\")\n",
    "print(f\"Dev: {len(dev)}\")\n",
    "print(f\"Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c2e8fba-8d92-40b8-b001-a7fe32d31a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': ['halapenimkkeyse', 'hakkyoey', 'kasinta'], 'translation': ['Grandfather', '(', 'HON', ')', 'goes', '(', 'HON', ')', 'to', 'school', '.'], 'glosses': ['grandfather(HON)', '-NOM(HON)', 'school', '-to', 'go', '-SH', '-PRES', '-DEC']}\n"
     ]
    }
   ],
   "source": [
    "print(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a540a049-bb55-4565-b40b-78f70077561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 3387\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "raw_dataset = DatasetDict()\n",
    "raw_dataset['train'] = Dataset.from_list(train)\n",
    "raw_dataset['validation'] = Dataset.from_list(dev)\n",
    "raw_dataset['test'] = Dataset.from_list(test)\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7779184-d767-4c62-91a6-c08c9b856ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 296,   57, 2774,  137,  983, 1442, 2317, 1887,    1, 4688, 4161, 5520,\n",
       "         5013, 6189, 4641, 5482, 5830, 3722,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([5830, 3556, 4641, 3525, 4688, 3525, 4146, 5520, 3610, 3377,    5,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2]),\n",
       " 'decoder_input_ids': tensor([   4, 5830, 3556, 4641, 3525, 4688, 3525, 4146, 5520, 3610, 3377,    5,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS_ID = special_chars.index(\"[BOS]\")\n",
    "EOS_ID = special_chars.index(\"[EOS]\")\n",
    "\n",
    "def preprocess(row):\n",
    "    \"\"\"Preprocesses each row in the dataset\n",
    "    1. Combines the source and translation into a single list, and encodes\n",
    "    2. Pads the combined input and output sequences\n",
    "    3. Creates attention mask\n",
    "    \"\"\"\n",
    "    source_enc = encode(row['words'])\n",
    "    transl_enc = encode(row['translation'], vocab='transl')\n",
    "    combined_enc = source_enc + [SEP_ID] + transl_enc\n",
    "    \n",
    "    # Pad\n",
    "    initial_length = len(combined_enc)\n",
    "    combined_enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = [1] * initial_length + [0] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    # Encode the output\n",
    "    output_enc = encode(row['glosses'], vocab='transl')\n",
    "    output_enc = output_enc + [EOS_ID]\n",
    "    \n",
    "    # Shift one position right\n",
    "    decoder_input_ids = [BOS_ID] + output_enc\n",
    "    \n",
    "    # Pad both\n",
    "    output_enc += [PAD_ID] * (MODEL_INPUT_LENGTH - len(output_enc))\n",
    "    decoder_input_ids += [PAD_ID] * (MODEL_INPUT_LENGTH - len(decoder_input_ids))\n",
    "    \n",
    "    return {'input_ids': torch.tensor(combined_enc), 'attention_mask': torch.tensor(attention_mask), 'labels': torch.tensor(output_enc), 'decoder_input_ids': torch.tensor(decoder_input_ids)}\n",
    "    \n",
    "preprocess(raw_dataset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d43ce5-b38a-45a9-be4e-8ad401abfec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e829b1306ae04532ab6c506352c854f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3387 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71866f35a3b491b8462bf848b2f65a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/726 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "452af29beb6d472589f3ad59ec99e053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/726 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'translation', 'glosses', 'input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
       "        num_rows: 3387\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'translation', 'glosses', 'input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'translation', 'glosses', 'input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to all datasets\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = raw_dataset['train'].map(preprocess)\n",
    "dataset['validation'] = raw_dataset['validation'].map(preprocess)\n",
    "dataset['test'] = raw_dataset['test'].map(preprocess)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e788be56-b2ce-4003-81c2-e4ccd294585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [\"b'\", '.', 'thoyoiley/', '??', 'i', 'kongcangi', 'pwuli', 'naessta'], 'translation': ['Fire', 'broke', 'out', 'in', 'the', 'factory', 'on', 'Saturday', '.'], 'glosses': ['Saturday', '-on/?NOM', 'factory', '-NOM', 'fire', '-NOM', 'break', 'out', '-PST', '-DEC'], 'input_ids': [296, 57, 2774, 137, 983, 1442, 2317, 1887, 1, 4688, 4161, 5520, 5013, 6189, 4641, 5482, 5830, 3722, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [5830, 3556, 4641, 3525, 4688, 3525, 4146, 5520, 3610, 3377, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'decoder_input_ids': [4, 5830, 3556, 4641, 3525, 4688, 3525, 4146, 5520, 3610, 3377, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7960f-74d5-40fc-a3a4-c185f33b2562",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dfec003a-6279-4a1d-902d-ba825d60ddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 4,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 4,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 5,\n",
       "  \"forced_eos_token_id\": 5,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"num_beams\": 5,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"scale_embedding\": false,\n",
       "  \"transformers_version\": \"4.21.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 6532\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartConfig, BartForConditionalGeneration\n",
    "\n",
    "config = BartConfig(\n",
    "    vocab_size=len(special_chars) + len(source_vocab) + len(target_and_gloss_vocab),\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=PAD_ID,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID,\n",
    "    decoder_start_token_id=BOS_ID,\n",
    "    forced_eos_token_id=EOS_ID,\n",
    "    num_beams = 5\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration(config)\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fffc1bcf-c938-4caa-8224-075ad88a8ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   4, 1515, 4434, 4434, 4434, 4434, 6376, 6376, 6376, 6376, 6376, 4434,\n",
       "         4434, 4434, 4434, 4434, 3485, 3485, 3485,    5]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.generate(torch.LongTensor([dataset[\"train\"][0]['input_ids']]), num_beams=5, min_length=0, max_length=20)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33774d55-f00e-4a9a-96af-ecb6fe6209cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['kumyoiley',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'was',\n",
       "  'was',\n",
       "  'was',\n",
       "  'was',\n",
       "  'was',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  'delicious',\n",
       "  '-kes',\n",
       "  '-kes',\n",
       "  '-kes']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocab = special_chars + source_vocab + target_and_gloss_vocab\n",
    "\n",
    "def batch_decode(batch):\n",
    "    \"\"\"Decodes a batch of indices to the actual words\"\"\"\n",
    "    def decode(seq):\n",
    "        if isinstance(seq, torch.Tensor):\n",
    "            indices = seq.detach().cpu().tolist()\n",
    "        else:\n",
    "            indices = seq.tolist()\n",
    "        return [all_vocab[index] for index in indices if index >= len(special_chars)]\n",
    "        \n",
    "    return [decode(seq) for seq in batch]\n",
    "        \n",
    "batch_decode(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a062c5b-bd5a-40b9-a4c3-81b3f3db8381",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53bb00ac-f73c-47ea-a840-a5ee47691979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "bleu_score(batch_decode(preds), [[dataset[\"train\"][0]['glosses']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56e562a7-d0c7-4f89-b004-391aa6410d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    # Decode predicted output\n",
    "    decoded_preds = batch_decode(preds)\n",
    "    \n",
    "    # Decode (gold) labels\n",
    "    labels = np.where(labels != -100, labels, PAD_ID)\n",
    "    decoded_labels = batch_decode(labels)\n",
    "    \n",
    "    bleu = bleu_score(decoded_preds, [[seq] for seq in decoded_labels])\n",
    "    \n",
    "    # Also get accuracy, based on (correct morphemes in output) / (len of correct output)\n",
    "    correct_glosses = 0\n",
    "    total_glosses = 0\n",
    "    \n",
    "    for (pred, labels) in zip(decoded_preds, decoded_labels):\n",
    "        correct_glosses += len([gloss for gloss in labels if gloss in pred ])\n",
    "        total_glosses += len(labels)\n",
    "    \n",
    "    acc = round(correct_glosses / total_glosses, 4)\n",
    "    \n",
    "    return {'bleu': bleu, 'accuracy': acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf38016-9faf-4636-b0b1-3a4118f408a3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39c93716-ea60-4495-b067-3f3c4fc453fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"igt-word-level\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a93b959-3073-4249-bc77-c803027850e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcdf0968-11c2-48c4-8897-b3cefcadf700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: words, glosses, translation. If words, glosses, translation are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/Users/milesper/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3387\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 212\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='212' max='212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [212/212 2:21:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.618670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: words, glosses, translation. If words, glosses, translation are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 726\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=212, training_loss=6.1274080096550705, metrics={'train_runtime': 8559.476, 'train_samples_per_second': 0.396, 'train_steps_per_second': 0.025, 'total_flos': 3669991643676672.0, 'train_loss': 6.1274080096550705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f4a2625-9626-42d5-bddb-2372cf954995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: glosses, translation, words. If glosses, translation, words are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 8.999423027038574,\n",
       " 'eval_bleu': 0.0,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 2.9284,\n",
       " 'eval_samples_per_second': 0.683,\n",
       " 'eval_steps_per_second': 0.341}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=dataset[\"validation\"].select([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9001f4a-ea77-4b04-bfba-714f65e094f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
