{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b8cba48-0d10-4dc4-97e2-aa6013a3f2d3",
   "metadata": {},
   "source": [
    "## Basic Model Training\n",
    "Let's get our data into a format that we can use to train transformer-style models.\n",
    "\n",
    "We will start with the Korean dataset, as it is well-annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "112cae38-79a3-401e-bbc2-9d075625c55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xigt in /Users/milesper/miniforge3/lib/python3.10/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e993ab-75ec-414e-bf97-c889c40210f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xigt.codecs import xigtxml\n",
    "corpus = xigtxml.load(open('../data/kor.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "247475e5-92cd-4837-977f-9b40752cc20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['ani', ',', 'caki-ka', 'cikcep', 'o-ess-ta', '.'],\n",
       " 'translation': ['No', ',', 'self', 'came', 'in', 'person', '.'],\n",
       " 'glosses': ['no', 'self-NOM', 'in-person', 'come-PAST-DEC']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MissingValueError(Exception):\n",
    "    pass\n",
    "\n",
    "# From a single line of IGT, extracts the features which are allowed in this shared task:\n",
    "# 1. Transcribed words (not segmented)\n",
    "# 2. Translation (not aligned)\n",
    "# 3. Glosses\n",
    "def extract_igt(igt):\n",
    "    if not igt.get('w'):\n",
    "        raise MissingValueError(\"words\")\n",
    "    if not igt.get('tw'):\n",
    "        raise MissingValueError(\"translation\")\n",
    "    if not igt.get('gw'):\n",
    "        raise MissingValueError(\"glosses\")\n",
    "        \n",
    "    words = [word.value() for word in igt['w'].items]\n",
    "    glosses = [gloss.value() for gloss in igt['gw'].items]\n",
    "    \n",
    "    translation = [item.value() for item in igt['tw']]\n",
    "    return {'words': words, 'translation': translation, 'glosses': glosses}\n",
    "    \n",
    "extract_igt(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9a7f38d-f50f-48b7-b744-a9cf339c8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed corpus, with \n",
      "\t4839 good rows\n",
      "\t73 rows missing words        \n",
      "\t471 missing translations\n",
      "\t0 missing glosses\n"
     ]
    }
   ],
   "source": [
    "corpus_data = []\n",
    "\n",
    "missing_words_count = 0\n",
    "missing_translation_count = 0\n",
    "missing_gloss_count = 0\n",
    "all_good_count = 0\n",
    "\n",
    "for i, igt in enumerate(corpus):\n",
    "    try:\n",
    "        igt_data = extract_igt(igt)\n",
    "        corpus_data.append(igt_data)\n",
    "        all_good_count += 1\n",
    "    except MissingValueError as v:\n",
    "        match str(v):\n",
    "            case 'words': missing_words_count += 1\n",
    "            case 'translation': missing_translation_count += 1\n",
    "            case 'glosses': missing_gloss_count += 1\n",
    "            case 'alignments': missing_aligments_count += 1\n",
    "    \n",
    "print(f\"Parsed corpus, with \\n\\t{all_good_count} good rows\\n\\t{missing_words_count} rows missing words\\\n",
    "        \\n\\t{missing_translation_count} missing translations\\n\\t{missing_gloss_count} missing glosses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6120a391-07e6-4a50-9959-06b016e5289c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsu-nun', 'pam-ul', 'kuw-e', 'mek-ess-ta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "752f6266-bec9-4525-a7d4-eab2b116cd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top', 'chestnut-Acc', 'broil-Inf', 'eat-Past-Dec']}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove the dashes from the input, to simulate the case where we don't have segmentation\n",
    "for item in corpus_data:\n",
    "    for i, word in enumerate(item['words']):\n",
    "        item['words'][i] = word.replace('-', '')\n",
    "        \n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a305452e-66c6-411e-b095-e22a8e5e50fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'words': ['Chelsunun', 'pamul', 'kuwe', 'mekessta', '.'],\n",
       " 'translation': ['Chelsu', 'broiled', 'and', 'ate', 'the', 'chestnut'],\n",
       " 'glosses': ['Top',\n",
       "  'chestnut',\n",
       "  '-Acc',\n",
       "  'broil',\n",
       "  '-Inf',\n",
       "  'eat',\n",
       "  '-Past',\n",
       "  '-Dec']}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's also split the output by dashes\n",
    "for item in corpus_data:\n",
    "    glosses = []\n",
    "    for i, word in enumerate(item['glosses']):\n",
    "        word_glosses = word.split(\"-\")\n",
    "        glosses.append(word_glosses[0])\n",
    "        glosses += [\"-\" + gloss for gloss in word_glosses[1:]]\n",
    "    item['glosses'] = glosses\n",
    "\n",
    "corpus_data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d75d8-1f32-4dd1-b27b-4092d5b1bf24",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- We originally tried to align words and glosses, but it turns out a huge number of rows are either missing alignments, or have completely wrong alignments. Rather than mess up our model with incorrect data, we will simply provide unaligned glosses.\n",
    "- There's a lot of messy unnecessary data. We will have to count on the transformer to deal with those.\n",
    "\n",
    "# Encoding\n",
    "Input: transcription + translation\n",
    "\n",
    "Output: glosses (stems and grams)\n",
    "\n",
    "We need to encode all of our items, input and output, as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f4840ca3-f319-453e-8004-faad3611b80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3169"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "special_chars = [\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\n",
    "def create_vocab(sentences: List[List[str]], threshold=2):\n",
    "    all_words = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            all_words[word.lower()] = all_words.get(word.lower(), 0) + 1\n",
    "\n",
    "    all_words_list = []\n",
    "    for word, count in all_words.items():\n",
    "        if count >= threshold:\n",
    "            all_words_list.append(word)\n",
    "\n",
    "    return sorted(all_words_list)\n",
    "\n",
    "source_vocab = create_vocab([item['words'] for item in corpus_data])\n",
    "len(source_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19b71530-6266-4400-9408-a026d66fa28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3357\n"
     ]
    }
   ],
   "source": [
    "# Also create a list for the target and gloss words\n",
    "target_and_gloss_vocab = create_vocab([item['translation'] for item in corpus_data] + [item['glosses'] for item in corpus_data])\n",
    "print(len(target_and_gloss_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17603cd6-05f6-49d6-8218-afdc7fb9494e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3175"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_word(word, vocab='source'):\n",
    "    word = word.lower()\n",
    "    \n",
    "    if word in special_chars:\n",
    "        return special_chars.index(word)\n",
    "    if vocab=='source':\n",
    "        if word in source_vocab:\n",
    "            return source_vocab.index(word) + len(special_chars)\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        if word in target_and_gloss_vocab:\n",
    "            return target_and_gloss_vocab.index(word) + len(special_chars) + len(source_vocab)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "encode_word('', vocab='transl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d96308c5-7265-472e-ba75-196d9327db67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[481, 2143, 1532, 1721, 57]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_INPUT_LENGTH = 512\n",
    "\n",
    "PAD_ID = special_chars.index(\"[PAD]\")\n",
    "SEP_ID = special_chars.index(\"[SEP]\")\n",
    "\n",
    "# Encodes a sentence as integers, and pads it\n",
    "def encode(sentence: List[str], vocab='source') -> List[int]:\n",
    "    return [encode_word(word, vocab=vocab) for word in sentence]\n",
    "            \n",
    "encode(corpus_data[4]['words']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d305e-0ffa-48ea-baa7-06614a0cdfc0",
   "metadata": {},
   "source": [
    "Now let's divide our data and turn it into the Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99c706df-d6a0-43bf-b1b8-67734a526ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3387\n",
      "Dev: 726\n",
      "Test: 726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(corpus_data, test_size=0.3)\n",
    "test, dev = train_test_split(test, test_size=0.5)\n",
    "\n",
    "print(f\"Train: {len(train)}\")\n",
    "print(f\"Dev: {len(dev)}\")\n",
    "print(f\"Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c2e8fba-8d92-40b8-b001-a7fe32d31a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': ['ce', 'seykayuy', 'cakun', 'inhyeng'], 'translation': ['Those', 'three', 'little', 'dolls'], 'glosses': ['DEM', 'three', '-CL', '-GEN', 'little', 'doll']}\n"
     ]
    }
   ],
   "source": [
    "print(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a540a049-bb55-4565-b40b-78f70077561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 3387\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['words', 'translation', 'glosses'],\n",
       "        num_rows: 726\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "raw_dataset = DatasetDict()\n",
    "raw_dataset['train'] = Dataset.from_list(train)\n",
    "raw_dataset['validation'] = Dataset.from_list(dev)\n",
    "raw_dataset['test'] = Dataset.from_list(test)\n",
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7779184-d767-4c62-91a6-c08c9b856ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([1878,  766,  140, 2024, 2970, 2092, 3091,  145,  540, 2978, 1301,  135,\n",
       "            1, 3216, 4939, 3944, 6510, 4793, 6241, 5170, 5000, 3185, 4875, 3198,\n",
       "         4324, 4602, 3781,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'decoder_input_ids': tensor([   4, 6482, 5460, 6310, 3663, 5460, 5527, 6360, 4939, 3515, 5460, 5247,\n",
       "         4324, 4531, 3452, 5170, 3420, 6130, 3322, 3787, 6360, 4838, 3824,    5,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(row):\n",
    "    \"\"\"Preprocesses each row in the dataset\n",
    "    1. Combines the source and translation into a single list, and encodes\n",
    "    2. Pads the combined input and output sequences\n",
    "    3. Creates attention mask\n",
    "    \"\"\"\n",
    "    source_enc = encode(row['words'])\n",
    "    transl_enc = encode(row['translation'], vocab='transl')\n",
    "    combined_enc = source_enc + [SEP_ID] + transl_enc\n",
    "    \n",
    "    # Pad\n",
    "    initial_length = len(combined_enc)\n",
    "    combined_enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = [1] * initial_length + [0] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    # Encode the output\n",
    "    output_enc = encode(row['glosses'], vocab='transl')\n",
    "    output_enc = [BOS_ID] + output_enc + [EOS_ID]\n",
    "    initial_length = len(output_enc)\n",
    "    output_enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "    return {'input_ids': torch.tensor(combined_enc), 'attention_mask': torch.tensor(attention_mask), 'decoder_input_ids': torch.tensor(output_enc)}\n",
    "    \n",
    "preprocess(raw_dataset['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a0d43ce5-b38a-45a9-be4e-8ad401abfec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8c68dc4ba9473e939a221ba052c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3387 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'translation', 'glosses', 'input_ids', 'attention_mask', 'decoder_input_ids'],\n",
       "    num_rows: 3387\n",
       "})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to all datasets\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = raw_dataset['train'].map(preprocess)\n",
    "dataset['validation'] = raw_dataset['validation'].map(preprocess)\n",
    "dataset['test'] = raw_dataset['train'].map(preprocess)\n",
    "\n",
    "# preprocess_all()\n",
    "# raw_dataset['train'][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0616eb24-d576-424e-a00f-72d146c7c512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  17,   79,   20,  ...,    2,    2,    2],\n",
       "         [ 242,   55,  339,  ...,    2,    2,    2],\n",
       "         [ 154,   57,  481,  ...,    2,    2,    2],\n",
       "         [ 295,   57, 1400,  ...,    2,    2,    2],\n",
       "         [ 481, 2143, 1532,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_batch(batch: List[List[List[str]]]):\n",
    "    \"\"\"Input should be a list of lists, where the first item is the source and the second is the translation\"\"\"\n",
    "    all_encoded = {'input_ids': [], 'attention_mask': []}\n",
    "    \n",
    "    for item in batch:\n",
    "        encoded = encode_item(item)\n",
    "        all_encoded['input_ids'].append(encoded['input_ids'])\n",
    "        all_encoded['attention_mask'].append(encoded['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.stack(all_encoded['input_ids'])\n",
    "    attention_mask = torch.stack(all_encoded['attention_mask'])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "encode_batch(corpus_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3149336d-a4c0-400e-ac8d-f0d0b0fdd7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   4, 3175, 3525,  ...,    2,    2,    2],\n",
      "        [   4, 5421, 5862,  ...,    2,    2,    2],\n",
      "        [   4, 4259, 3683,  ...,    2,    2,    2],\n",
      "        [   4, 4705, 3525,  ...,    2,    2,    2],\n",
      "        [   4, 6265, 4264,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "BOS_ID = special_chars.index(\"[BOS]\")\n",
    "EOS_ID = special_chars.index(\"[EOS]\")\n",
    "\n",
    "def encode_gloss_batch(batch: List[List[str]]):\n",
    "    \"\"\"Encodes an output batch. Each item should be a list of output gloss words.\"\"\"\n",
    "    all_encoded = {'input_ids': [], 'attention_mask': []}\n",
    "    for item in batch:\n",
    "        enc = encode(item, vocab='transl')\n",
    "        enc = [BOS_ID] + enc + [EOS_ID]\n",
    "        initial_length = len(enc)\n",
    "        enc += [PAD_ID] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "    \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * initial_length + [0] * (MODEL_INPUT_LENGTH - initial_length)\n",
    "        all_encoded['input_ids'].append(torch.tensor(enc))\n",
    "        all_encoded['attention_mask'].append(torch.tensor(attention_mask))\n",
    "        \n",
    "    return {'input_ids': torch.stack(all_encoded['input_ids']), 'attention_mask': torch.stack(all_encoded['attention_mask'])}\n",
    "\n",
    "print(encode_gloss_batch([item[2] for item in corpus_data[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ec7f965-2eba-4f04-8ce7-f3141ad5f07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  17,   79,   20,  ...,    2,    2,    2],\n",
       "         [ 242,   55,  339,  ...,    2,    2,    2],\n",
       "         [ 154,   57,  481,  ...,    2,    2,    2],\n",
       "         ...,\n",
       "         [1169,    0,    1,  ...,    2,    2,    2],\n",
       "         [ 140, 2358, 1154,  ...,    2,    2,    2],\n",
       "         [1676,  140, 2358,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "        \n",
    "batch_size = 64\n",
    "\n",
    "train_input_batches = [encode_batch(b) for b in chunk(corpus_data, batch_size)]\n",
    "train_input_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a4de496-a3cb-4246-9c78-50628321f0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   4, 3175, 3525,  ...,    2,    2,    2],\n",
       "         [   4, 5421, 5862,  ...,    2,    2,    2],\n",
       "         [   4, 4259, 3683,  ...,    2,    2,    2],\n",
       "         ...,\n",
       "         [   4, 5077, 3425,  ...,    2,    2,    2],\n",
       "         [   4, 5077, 3525,  ...,    2,    2,    2],\n",
       "         [   4, 5278, 3525,  ...,    2,    2,    2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same for output\n",
    "train_output_batches = [encode_gloss_batch(b) for b in chunk([item[2] for item in corpus_data], batch_size)]\n",
    "train_output_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a7960f-74d5-40fc-a3a4-c185f33b2562",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfec003a-6279-4a1d-902d-ba825d60ddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 4,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 1024,\n",
       "  \"decoder_attention_heads\": 16,\n",
       "  \"decoder_ffn_dim\": 4096,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 12,\n",
       "  \"decoder_start_token_id\": 4,\n",
       "  \"dropout\": 0.1,\n",
       "  \"encoder_attention_heads\": 16,\n",
       "  \"encoder_ffn_dim\": 4096,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 12,\n",
       "  \"eos_token_id\": 5,\n",
       "  \"forced_eos_token_id\": 5,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 2,\n",
       "  \"scale_embedding\": false,\n",
       "  \"transformers_version\": \"4.21.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 6526\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartConfig, BartForConditionalGeneration\n",
    "\n",
    "config = BartConfig(\n",
    "    vocab_size=len(source_vocab) + len(target_and_gloss_vocab),\n",
    "    max_position_embeddings=512,\n",
    "    pad_token_id=PAD_ID,\n",
    "    bos_token_id=BOS_ID,\n",
    "    eos_token_id=EOS_ID,\n",
    "    decoder_start_token_id=BOS_ID,\n",
    "    forced_eos_token_id=EOS_ID\n",
    ")\n",
    "\n",
    "model = BartForConditionalGeneration(config)\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fffc1bcf-c938-4caa-8224-075ad88a8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm our model works with our data\n",
    "# model(input_ids=train_input_batches[0]['input_ids'],\n",
    "#       attention_mask=train_input_batches[0]['attention_mask'],\n",
    "#       decoder_input_ids=train_output_batches[0]['input_ids'],\n",
    "#       decoder_attention_mask=train_output_batches[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf38016-9faf-4636-b0b1-3a4118f408a3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39c93716-ea60-4495-b067-3f3c4fc453fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"igt-word-level\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93b959-3073-4249-bc77-c803027850e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
